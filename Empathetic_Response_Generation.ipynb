{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e00c3b0",
   "metadata": {},
   "source": [
    "**Yong Zhu Cheng A0275768H**\n",
    "\n",
    "The objective of the project is to train a machine to generate an empathetic response to a given prompt, leveraging the large EmpatheticDialogues dataset developed by Rashkin et al.(2018). The dataset consists of crowd-sourced one-on-one dialogues covering a range of emotions, with human-annotated emotions as a key feature.\n",
    "\n",
    "The model used is a pre-trained version of BART, an encoder-decoder architecture that was pre-trained on the principle of denoising documents. This approach is different from the one proposed in my HW2 submission, as an extremely large amount of data and time would have been needed to train a transformer from scratch. On other hand, the pretrained model readily lends itself to interesting sequence-to-sequence applications, which makes it a more feasible subject of experimentation and study in this short span of time.\n",
    "\n",
    "## 1. Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9afcbe9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import tarfile\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertGenerationEncoder, BertGenerationDecoder, EncoderDecoderModel\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from transformers import Seq2SeqTrainingArguments, Trainer\n",
    "from datasets import Dataset\n",
    "from pynvml import *\n",
    "import evaluate\n",
    "import warnings\n",
    "import gc\n",
    "\n",
    "main_path = r'/home/jupyter/DSA5202_Project'\n",
    "os.chdir(main_path)\n",
    "device = 'cuda' if torch.cuda.is_available() else'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e1ad9c5-81ee-48a8-b937-002d4f8f97ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def get_gpu_usage():\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    usage = info.used//1024**2\n",
    "    return usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d44e4de",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing\n",
    "**Discussion of Dataset:**\n",
    "\n",
    "The dataset is a collection of annotated personal dialogues. For this project, I re-organized the data into a set of input-output pairs, where each input and output are respectively defined as sequence(i-1) and sequence(i) in a dialogue. I have kept the emotion feature for further use, as well as removed the first turn of every dialogue (i.e. sequence(0)) and dialogues with any missing turns (e.g. [1,2,4,5]). \n",
    "\n",
    "**The following cells, up to the indicated checkpoint, are only run once:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "680fcf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # [RUN ONCE] Extract zip file\n",
    "# data_path = r'dataset'\n",
    "# file = tarfile.open('empatheticdialogues.tar.gz')\n",
    "# if os.path.isdir(data_path) is False:\n",
    "#     os.mkdir(data_path)\n",
    "#     file.extractall(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25830e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(r'dataset/empatheticdialogues/train.csv',on_bad_lines='skip')\n",
    "test_df = pd.read_csv(r'dataset/empatheticdialogues/test.csv',on_bad_lines='skip')\n",
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3078cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bfb75eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prev_utterance(row,df):\n",
    "    if row.utterance_idx == 1:\n",
    "        return pd.NA\n",
    "    else:\n",
    "        return df.at[row.conv_id+'_'+str(int(row.utterance_idx)-1),'utterance']\n",
    "\n",
    "def process_df(df0,save_dir):    \n",
    "    df = df0.copy()\n",
    "    df['index1'] = df['conv_id'] + '_' + df['utterance_idx'].astype(str)\n",
    "    df = df.set_index('index1',drop=True)\n",
    "    conv_ids = df['conv_id'].unique()\n",
    "    for conv_idx in tqdm(conv_ids):\n",
    "        u_ids = [df.at[i,'utterance_idx'] for i in df.index if conv_idx in i]\n",
    "        if max(u_ids) != len(u_ids):\n",
    "            df = df.drop(df[df['conv_id']==conv_idx].index)\n",
    "    df['utterance0'] = df.apply(lambda row: get_prev_utterance(row,df),axis=1)\n",
    "    df = df.reset_index(drop=True)\n",
    "    df = df[['context','prompt','utterance0','utterance']]\n",
    "    df = df.rename(columns={'utterance':'utterance1'})\n",
    "    df = df.dropna(subset='utterance0')\n",
    "    df.to_csv(save_dir,index=False)\n",
    "    \n",
    "process_df(train_df,r'dataset/empatheticdialogues/train_v1.csv')\n",
    "process_df(test_df,r'dataset/empatheticdialogues/test_v1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3decc0e8",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Resume from here:**\n",
    "## 3. Model Building\n",
    "**Overview of Model:**\n",
    "\n",
    "BART (Lewis et al., 2019) is a transfomer-based encoder-decoder architecture with a training methodology based on denoising transformed input sequences. Such noising transformations include text infilling and masking as well as sequence rotations and permutations, which seem to help it generate and 'comprehend' texts reasonably well.\n",
    "\n",
    "Structurally, the model uses a standard bidirectional encoder and a left-to-right decoder. Its choice of absolute positional embeddings over relative embeddings is also why I have used right-side padding for my tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98eb7bd5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>prompt</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sentimental</td>\n",
       "      <td>I remember going to the fireworks with my best...</td>\n",
       "      <td>I remember going to see the fireworks with my ...</td>\n",
       "      <td>Was this a friend you were in love with_comma_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sentimental</td>\n",
       "      <td>I remember going to the fireworks with my best...</td>\n",
       "      <td>Was this a friend you were in love with_comma_...</td>\n",
       "      <td>This was a best friend. I miss her.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sentimental</td>\n",
       "      <td>I remember going to the fireworks with my best...</td>\n",
       "      <td>This was a best friend. I miss her.</td>\n",
       "      <td>Where has she gone?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sentimental</td>\n",
       "      <td>I remember going to the fireworks with my best...</td>\n",
       "      <td>Where has she gone?</td>\n",
       "      <td>We no longer talk.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sentimental</td>\n",
       "      <td>I remember going to the fireworks with my best...</td>\n",
       "      <td>We no longer talk.</td>\n",
       "      <td>Oh was this something that happened because of...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       context                                             prompt  \\\n",
       "0  sentimental  I remember going to the fireworks with my best...   \n",
       "1  sentimental  I remember going to the fireworks with my best...   \n",
       "2  sentimental  I remember going to the fireworks with my best...   \n",
       "3  sentimental  I remember going to the fireworks with my best...   \n",
       "4  sentimental  I remember going to the fireworks with my best...   \n",
       "\n",
       "                                                text  \\\n",
       "0  I remember going to see the fireworks with my ...   \n",
       "1  Was this a friend you were in love with_comma_...   \n",
       "2                This was a best friend. I miss her.   \n",
       "3                                Where has she gone?   \n",
       "4                                 We no longer talk.   \n",
       "\n",
       "                                               label  \n",
       "0  Was this a friend you were in love with_comma_...  \n",
       "1                This was a best friend. I miss her.  \n",
       "2                                Where has she gone?  \n",
       "3                                 We no longer talk.  \n",
       "4  Oh was this something that happened because of...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df1 = pd.read_csv(r'dataset/empatheticdialogues/train_v1.csv')\n",
    "test_df1 = pd.read_csv(r'dataset/empatheticdialogues/test_v1.csv')\n",
    "obj_del = ['enc','dec','model','tokenizer']\n",
    "train_df1 = train_df1.rename(columns={'utterance0':'text','utterance1':'label'})\n",
    "train_df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c94ac24-5910-460c-a6ea-0c3d81fcc171",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 69.92530202484261 640 44.98942194615673\n"
     ]
    }
   ],
   "source": [
    "text_len = train_df1['text'].str.len().to_numpy()\n",
    "print(np.min(text_len),np.average(text_len),np.max(text_len),np.std(text_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b2c223",
   "metadata": {},
   "source": [
    "*Model 1 [Experiment]: BERT-based Encoder-Decoder*\n",
    "\n",
    "This was the first model that I tried, which I then discarded for a pretrained S2S architecture. \n",
    "\n",
    "*Try not to run or build both BERT-based and BART architectures together on one kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0aa4bbdb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model_checkpoint = 'google-bert/bert-base-uncased'\n",
    "# for obj in obj_del:\n",
    "#     if obj in globals(): del globals()[obj]\n",
    "# with warnings.catch_warnings():\n",
    "#     warnings.simplefilter('ignore')\n",
    "#     tokenizer = BertTokenizer.from_pretrained(model_checkpoint,do_lower_case=True)\n",
    "# def tokenize(sample):\n",
    "#     return tokenizer(sample['text'],text_target=sample['label'],max_length=200,padding='max_length',truncation=True)\n",
    "\n",
    "# core_ds = Dataset.from_pandas(train_df1[['text','label']])\n",
    "# core_ds = core_ds.map(tokenize,batched=True,remove_columns=core_ds.column_names)\n",
    "# core_ds = core_ds.train_test_split(test_size=0.2,seed=10)\n",
    "# train_ds = core_ds['train'].select(range(2000)) # for smaller subset\n",
    "# val_ds =  core_ds['test'].select(range(1000))\n",
    "# enc = BertGenerationEncoder.from_pretrained(model_checkpoint)\n",
    "# dec = BertGenerationDecoder.from_pretrained(model_checkpoint, add_cross_attention=True, is_decoder=True, bos_token_id=101, eos_token_id=102)\n",
    "# model = EncoderDecoderModel(encoder=enc,decoder=dec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde8ebc0",
   "metadata": {},
   "source": [
    "*Model 2 [Selected]: BART Architecture*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0d5a40ac",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddd46970b9544a52ab501601ad36f9f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/58770 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_checkpoint = 'facebook/bart-base'\n",
    "for obj in obj_del:\n",
    "    if obj in globals(): del globals()[obj]\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint,do_lower_case=True)\n",
    "def tokenize(sample):\n",
    "    return tokenizer(sample['text'],text_target=sample['label'],max_length=200,padding='max_length',truncation=True)\n",
    "def tokenize_tagged(sample): # Not ideal\n",
    "    return tokenizer(sample['text_tagged'],text_target=sample['label'],max_length=200,padding='max_length',truncation=True)\n",
    "def process_ds(df,text_col='text'):\n",
    "    core_ds = Dataset.from_pandas(df[[text_col,'label']])\n",
    "    if text_col == 'text':\n",
    "        core_ds = core_ds.map(tokenize,batched=True,remove_columns=core_ds.column_names)\n",
    "    elif text_col == 'text_tagged':\n",
    "        core_ds = core_ds.map(tokenize_tagged,batched=True,remove_columns=core_ds.column_names)\n",
    "    core_ds = core_ds.train_test_split(test_size=0.2,seed=10)\n",
    "    train_ds = core_ds['train'].select(range(2000)) # for smaller subset\n",
    "    val_ds =  core_ds['test'].select(range(1000))\n",
    "    return core_ds, train_ds, val_ds\n",
    "core_ds, train_ds, val_ds = process_ds(train_df1)\n",
    "pre_model_gpu = get_gpu_usage()\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "11d32d40-b392-4db1-b99c-5c7fa8a3948b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_layers: 6 decoder_layers: 6 activation funcion: gelu\n",
      "encoder_attention_heads: 12 decoder_attention_heads: 12 attention_dropout: 0.1\n",
      "d_model: 768 encoder_ffn_dim: 3072 decoder_ffn_dim: 3072\n",
      "num_parameters: 139420416 model size(MB): 3282\n"
     ]
    }
   ],
   "source": [
    "print('encoder_layers:',model.config.encoder_layers,'decoder_layers:',model.config.decoder_layers,'activation funcion:',model.config.activation_function)\n",
    "print('encoder_attention_heads:',model.config.encoder_attention_heads,'decoder_attention_heads:',model.config.decoder_attention_heads,'attention_dropout:',model.config.attention_dropout)\n",
    "print('d_model:',model.config.d_model,'encoder_ffn_dim:',model.config.encoder_ffn_dim,'decoder_ffn_dim:',model.config.decoder_ffn_dim)\n",
    "model.to(device)\n",
    "print('num_parameters:',model.num_parameters(),'model size(MB):',get_gpu_usage()-pre_model_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0002e3b4-9bc0-4931-81fb-27b903cf211a",
   "metadata": {},
   "source": [
    "**Discussion of Model:**\n",
    "\n",
    "From the above, we observe that this is a large deep large model with 1.2GB in memory usage. There is significant depth and breadth, with 6 layers apiece for both the encoder and decoder, as well as 3072 nodes in the feed-forward layers. The authors also went with 12 heads per multi-attention block, which should help the model capture richer inter-token relationships. Also, in addition to the noising transformations used in pre-training, other efforts at regularization include the use of dropout in the attention layers as well as early stopping. While it is difficult to attribute the efficacy of the model to any of its specific components, the combination of the above factors would certainly have helped in aggregate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99215abd-a30a-4dc7-a19b-eae32b56776a",
   "metadata": {},
   "source": [
    "## 4. Model Training and Evaluation\n",
    "### 4.1 Key Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5bb9d771",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_dir = r'models'\n",
    "bleu = evaluate.load('bleu')\n",
    "rouge = evaluate.load('rouge')\n",
    "if os.path.isdir(model_dir) is False:\n",
    "    os.mkdir(model_dir)\n",
    "def preprocess_logits_for_metrics(logits, labels):\n",
    "    \"\"\"\n",
    "    Workaround for OOM issue, due to potential memory leak in original Trainer class\n",
    "    \"\"\"\n",
    "    pred_ids = torch.argmax(logits[0], dim=-1)\n",
    "    return pred_ids, labels\n",
    "def compute_metrics(eval_pred):\n",
    "    preds,labels = eval_pred\n",
    "    if isinstance(preds,tuple): preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds,skip_special_tokens=True)\n",
    "    labels = np.where(labels!=-100,labels,tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels,skip_special_tokens=True)\n",
    "    decoded_preds = [P.strip() for P in decoded_preds]\n",
    "    decoded_labels = [L.strip() for L in decoded_labels]    \n",
    "    bleu_1 = bleu.compute(predictions=decoded_preds,references=decoded_labels)\n",
    "    rouge_1 = rouge.compute(predictions=decoded_preds,references=decoded_labels)\n",
    "    return {'bleu':bleu_1,'rouge':rouge_1}\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=model_dir,\n",
    "    disable_tqdm=False,\n",
    "    num_train_epochs=5,\n",
    "    predict_with_generate=True,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    eval_accumulation_steps=5,\n",
    "    fp16=True,\n",
    ") # finetune hp\n",
    "training_args.set_logging(report_to=['tensorboard'])\n",
    "training_args.set_dataloader(train_batch_size=2,eval_batch_size=2)\n",
    "\n",
    "model.config.decoder_start_token_id = model.config.bos_token_id\n",
    "model.config.pad_token_id = -100 # https://discuss.huggingface.co/t/expected-workflow-100-and-padding-in-labels-in-seq2seq/27692\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    "    preprocess_logits_for_metrics=preprocess_logits_for_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ff79cd-2572-48b4-9a9b-fce020ca0e80",
   "metadata": {},
   "source": [
    "**Notes on Model Training:**\n",
    "\n",
    "A few interventions were implemented to ensure sufficient memory for model training. These include:\n",
    "- Reducing batch-size and sample size.\n",
    "- Enabling DDP - splitting each batch among 2 x 16GB GPUS on GCP.\n",
    "- Sending smaller prediction vectors to CPU for evaluation by simply sending the largest logit per row to the CPU. \n",
    "- Reducing eval_accumulation_steps so that prediction values over fewer steps are sent to the CPU each time.\n",
    "- Sequence length set as 200 - more than 3 standard deviations above the mean sequence length."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0162fbad-2dc6-4c63-8194-72ae26696636",
   "metadata": {},
   "source": [
    "### 4.2 Evaluation of Pre-Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "986de6ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 16:56]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"{'bleu': 0.0026524365368443847, 'precisions': [0.04834689057990029, 0.00824248869981388, 0.001818916734033953, 6.828735318219065e-05], 'brevity_penalty': 1.0, 'length_ratio': 4.789192585611059, 'translation_length': 15244, 'reference_length': 3183}\" of type <class 'dict'> for key \"eval/bleu\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'rouge1': 0.1318835069428388, 'rouge2': 0.016720179329673776, 'rougeL': 0.11423932647072439, 'rougeLsum': 0.11388708462868741}\" of type <class 'dict'> for key \"eval/rouge\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 18.297039031982422,\n",
       " 'eval_bleu': {'bleu': 0.0026524365368443847,\n",
       "  'precisions': [0.04834689057990029,\n",
       "   0.00824248869981388,\n",
       "   0.001818916734033953,\n",
       "   6.828735318219065e-05],\n",
       "  'brevity_penalty': 1.0,\n",
       "  'length_ratio': 4.789192585611059,\n",
       "  'translation_length': 15244,\n",
       "  'reference_length': 3183},\n",
       " 'eval_rouge': {'rouge1': 0.1318835069428388,\n",
       "  'rouge2': 0.016720179329673776,\n",
       "  'rougeL': 0.11423932647072439,\n",
       "  'rougeLsum': 0.11388708462868741},\n",
       " 'eval_runtime': 6.9297,\n",
       " 'eval_samples_per_second': 28.861,\n",
       " 'eval_steps_per_second': 7.215}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()\n",
    "trainer.args.dataloader_num_workers = 0\n",
    "trainer.args.dataloader_prefetch_factor = 2 # To avoid HF error caused by setting prefetch_factor to None by default\n",
    "val_ds2 = core_ds['test'].select(range(200))\n",
    "trainer.evaluate(val_ds2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2ab6cc6b-9352-42ca-89c4-c8e512c87cd4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: That must be incredibly annoying to have to deal with. To constantly have to battle for what you feel is justified support_comma_ and to always be worrying about it in the back of your head. You have to be really resilient to go through that. \n",
      "Generated response: That must be incredibly annoying to have to deal with. To constantly have to battle for \n",
      "Actual response: Its been ten years and it doesnt even show up half of the time. I told the judge the last time I don't even want it anymore.\n"
     ]
    }
   ],
   "source": [
    "def generate_sentences(model,idx,ds):\n",
    "    sample_output = model.generate(torch.LongTensor(ds[idx]['input_ids']).unsqueeze(0).to(device))\n",
    "    sample_output = tokenizer.batch_decode(sample_output,skip_special_tokens=True)\n",
    "    sample_input = ds[idx]['input_ids']\n",
    "    sample_input = tokenizer.batch_decode(sample_input,skip_special_tokens=True)\n",
    "    sample_input = [w for w in sample_input if w != '']\n",
    "    sample_labels = ds[idx]['labels']\n",
    "    sample_labels = np.where(sample_labels!=-100,sample_labels,tokenizer.pad_token_id)\n",
    "    sample_labels = tokenizer.batch_decode(sample_labels,skip_special_tokens=True)\n",
    "    print('Prompt:',''.join(sample_input),'\\nGenerated response:',''.join(sample_output),'\\nActual response:',''.join(sample_labels))\n",
    "generate_sentences(model,0,train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de215255-3698-46bf-816b-cfbf18f39f6f",
   "metadata": {},
   "source": [
    "### 4.3 Training and Evaluation of Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "30186f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2500' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2500/2500 15:32, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.918400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.249100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.211300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.183300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.163600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "Your generation config was originally created from the model config, but the model config has changed since then. Unless you pass the `generation_config` argument to this model's `generate` calls, they will revert to the legacy behavior where the base `generate` parameterization is loaded from the model config instead. To avoid this behavior and this warning, we recommend you to overwrite the generation config model attribute before calling the model's `save_pretrained`, preferably also removing any generation kwargs from the model config. This warning will be raised to an exception in v4.41.\n",
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "Your generation config was originally created from the model config, but the model config has changed since then. Unless you pass the `generation_config` argument to this model's `generate` calls, they will revert to the legacy behavior where the base `generate` parameterization is loaded from the model config instead. To avoid this behavior and this warning, we recommend you to overwrite the generation config model attribute before calling the model's `save_pretrained`, preferably also removing any generation kwargs from the model config. This warning will be raised to an exception in v4.41.\n",
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "Your generation config was originally created from the model config, but the model config has changed since then. Unless you pass the `generation_config` argument to this model's `generate` calls, they will revert to the legacy behavior where the base `generate` parameterization is loaded from the model config instead. To avoid this behavior and this warning, we recommend you to overwrite the generation config model attribute before calling the model's `save_pretrained`, preferably also removing any generation kwargs from the model config. This warning will be raised to an exception in v4.41.\n",
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "Your generation config was originally created from the model config, but the model config has changed since then. Unless you pass the `generation_config` argument to this model's `generate` calls, they will revert to the legacy behavior where the base `generate` parameterization is loaded from the model config instead. To avoid this behavior and this warning, we recommend you to overwrite the generation config model attribute before calling the model's `save_pretrained`, preferably also removing any generation kwargs from the model config. This warning will be raised to an exception in v4.41.\n",
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "Your generation config was originally created from the model config, but the model config has changed since then. Unless you pass the `generation_config` argument to this model's `generate` calls, they will revert to the legacy behavior where the base `generate` parameterization is loaded from the model config instead. To avoid this behavior and this warning, we recommend you to overwrite the generation config model attribute before calling the model's `save_pretrained`, preferably also removing any generation kwargs from the model config. This warning will be raised to an exception in v4.41.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2500, training_loss=0.3451532043457031, metrics={'train_runtime': 933.0898, 'train_samples_per_second': 10.717, 'train_steps_per_second': 2.679, 'total_flos': 1190891520000000.0, 'train_loss': 0.3451532043457031, 'epoch': 5.0})"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.args.dataloader_num_workers = 0\n",
    "trainer.args.dataloader_prefetch_factor = 2 \n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2396e448-c7a1-48a6-b79d-43ae4d5f5e6f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "Your generation config was originally created from the model config, but the model config has changed since then. Unless you pass the `generation_config` argument to this model's `generate` calls, they will revert to the legacy behavior where the base `generate` parameterization is loaded from the model config instead. To avoid this behavior and this warning, we recommend you to overwrite the generation config model attribute before calling the model's `save_pretrained`, preferably also removing any generation kwargs from the model config. This warning will be raised to an exception in v4.41.\n"
     ]
    }
   ],
   "source": [
    "submission_model_dir = r'submission_model'\n",
    "if os.path.isdir(submission_model_dir) is False:\n",
    "    os.mkdir(submission_model_dir)\n",
    "trainer.save_model(submission_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0d2e1adb-207c-4872-a37f-28a7e4afc9c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Trainer is attempting to log a value of \"{'bleu': 0.07133228458970757, 'precisions': [0.38844042621892155, 0.11908871246116673, 0.04338153503893215, 0.014417300760913096], 'brevity_penalty': 0.9726132006914829, 'length_ratio': 0.9729814640276468, 'translation_length': 3097, 'reference_length': 3183}\" of type <class 'dict'> for key \"eval/bleu\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'rouge1': 0.35157515187169935, 'rouge2': 0.0897351333385348, 'rougeL': 0.32234644204153207, 'rougeLsum': 0.32162831666980013}\" of type <class 'dict'> for key \"eval/rouge\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.3026095926761627,\n",
       " 'eval_bleu': {'bleu': 0.07133228458970757,\n",
       "  'precisions': [0.38844042621892155,\n",
       "   0.11908871246116673,\n",
       "   0.04338153503893215,\n",
       "   0.014417300760913096],\n",
       "  'brevity_penalty': 0.9726132006914829,\n",
       "  'length_ratio': 0.9729814640276468,\n",
       "  'translation_length': 3097,\n",
       "  'reference_length': 3183},\n",
       " 'eval_rouge': {'rouge1': 0.35157515187169935,\n",
       "  'rouge2': 0.0897351333385348,\n",
       "  'rougeL': 0.32234644204153207,\n",
       "  'rougeLsum': 0.32162831666980013},\n",
       " 'eval_runtime': 6.7441,\n",
       " 'eval_samples_per_second': 29.656,\n",
       " 'eval_steps_per_second': 7.414,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(val_ds2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2e405479-ef9b-46f4-9cd5-0b45507def55",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: That must be incredibly annoying to have to deal with. To constantly have to battle for what you feel is justified support_comma_ and to always be worrying about it in the back of your head. You have to be really resilient to go through that. \n",
      "Generated response: I agree_comma_ but I have to deal with that every day. \n",
      "Actual response: Its been ten years and it doesnt even show up half of the time. I told the judge the last time I don't even want it anymore.\n"
     ]
    }
   ],
   "source": [
    "generate_sentences(model,0,train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff6e8f8-c15f-4374-868d-30f7d8d8f411",
   "metadata": {},
   "source": [
    "**Discussion of Model Results:**\n",
    "\n",
    "*Before Fine-Tuning:*\n",
    "\n",
    "{'eval_loss': 18.297039031982422,\n",
    " 'eval_bleu': {'bleu': 0.0026524365368443847,\n",
    "  'precisions': [0.04834689057990029,\n",
    "   0.00824248869981388,\n",
    "   0.001818916734033953,\n",
    "   6.828735318219065e-05],\n",
    "  'brevity_penalty': 1.0,\n",
    "  'length_ratio': 4.789192585611059,\n",
    "  'translation_length': 15244,\n",
    "  'reference_length': 3183},\n",
    " 'eval_rouge': {'rouge1': 0.1318835069428388,\n",
    "  'rouge2': 0.016720179329673776,\n",
    "  'rougeL': 0.11423932647072439,\n",
    "  'rougeLsum': 0.11388708462868741},\n",
    " 'eval_runtime': 6.9297,\n",
    " 'eval_samples_per_second': 28.861,\n",
    " 'eval_steps_per_second': 7.215}\n",
    " \n",
    " *After Fine-Tuning:*\n",
    " \n",
    " {'eval_loss': 0.3026095926761627,\n",
    " 'eval_bleu': {'bleu': 0.07133228458970757,\n",
    "  'precisions': [0.38844042621892155,\n",
    "   0.11908871246116673,\n",
    "   0.04338153503893215,\n",
    "   0.014417300760913096],\n",
    "  'brevity_penalty': 0.9726132006914829,\n",
    "  'length_ratio': 0.9729814640276468,\n",
    "  'translation_length': 3097,\n",
    "  'reference_length': 3183},\n",
    " 'eval_rouge': {'rouge1': 0.35157515187169935,\n",
    "  'rouge2': 0.0897351333385348,\n",
    "  'rougeL': 0.32234644204153207,\n",
    "  'rougeLsum': 0.32162831666980013},\n",
    " 'eval_runtime': 6.7441,\n",
    " 'eval_samples_per_second': 29.656,\n",
    " 'eval_steps_per_second': 7.414,\n",
    " 'epoch': 5.0}\n",
    "\n",
    "To evaluate model performance, I have measured the model's bleu and rouge scores pre- and post-fine-tuning, where bleu and rouge are respectively precision- and recall-based scores. Overall, we see a distinct improvement across all scores after fine-tuning, although it is also important to qualitatively evaluate the responses as bleu and rouge can be rather blunt measures (as they count direct ngram/token matches instead of measuring semantic similarity). That said, the model's responses seem to show some relevance and coherence, which supports the conclusion drawn from the metrics. In general, the pre-trained model simply parrots some part of the input prompt as its response (see the above examples), whereas the fine-tuned model appears capable of generating a original and coherent reply to the prompt, even if it may not be extremely 'empathetic' or appropriate at the moment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9777ed9-a630-43fb-82ae-da17494106fc",
   "metadata": {},
   "source": [
    "### 4.4 Reload Fine-Tuned Model\n",
    "For any purposes - evaluation, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "38ba3cc2-e8e9-4232-bd50-f747a4dbe406",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"{'bleu': 0.07133228458970757, 'precisions': [0.38844042621892155, 0.11908871246116673, 0.04338153503893215, 0.014417300760913096], 'brevity_penalty': 0.9726132006914829, 'length_ratio': 0.9729814640276468, 'translation_length': 3097, 'reference_length': 3183}\" of type <class 'dict'> for key \"eval/bleu\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'rouge1': 0.35157515187169935, 'rouge2': 0.0897351333385348, 'rougeL': 0.32234644204153207, 'rougeLsum': 0.32162831666980013}\" of type <class 'dict'> for key \"eval/rouge\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.3026095926761627,\n",
       " 'eval_bleu': {'bleu': 0.07133228458970757,\n",
       "  'precisions': [0.38844042621892155,\n",
       "   0.11908871246116673,\n",
       "   0.04338153503893215,\n",
       "   0.014417300760913096],\n",
       "  'brevity_penalty': 0.9726132006914829,\n",
       "  'length_ratio': 0.9729814640276468,\n",
       "  'translation_length': 3097,\n",
       "  'reference_length': 3183},\n",
       " 'eval_rouge': {'rouge1': 0.35157515187169935,\n",
       "  'rouge2': 0.0897351333385348,\n",
       "  'rougeL': 0.32234644204153207,\n",
       "  'rougeLsum': 0.32162831666980013},\n",
       " 'eval_runtime': 6.6985,\n",
       " 'eval_samples_per_second': 29.858,\n",
       " 'eval_steps_per_second': 7.464}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(submission_model_dir)\n",
    "val_ds2 = core_ds['test'].select(range(200))\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    "    preprocess_logits_for_metrics=preprocess_logits_for_metrics\n",
    ")\n",
    "trainer.args.dataloader_num_workers = 0\n",
    "trainer.args.dataloader_prefetch_factor = 2\n",
    "trainer.evaluate(val_ds2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d875b284-7fb7-4913-aeba-383060a11b3e",
   "metadata": {},
   "source": [
    "## 5. Further Improvements\n",
    "Note that the following improvements are performed on the **pre-trained model** to provide a common basis of comparison with the results from section 4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9baf86fd-a071-436e-be40-a937d6fa08ab",
   "metadata": {},
   "source": [
    "### 5.1 Feature Improvement: Including Emotional Context\n",
    "**Hypothesis:**\n",
    "\n",
    "Technically, the emotional context of each dialogue should provide essential information about the emotional state of the affected person and how best to respond to him/her."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "30333ae5-f9a2-4283-9277-670fcc6d8e06",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6f80844bad84beba8b03b41dd910a82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/58770 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2500' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2500/2500 15:33, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.907000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.248100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.210500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.182000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.162800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "Your generation config was originally created from the model config, but the model config has changed since then. Unless you pass the `generation_config` argument to this model's `generate` calls, they will revert to the legacy behavior where the base `generate` parameterization is loaded from the model config instead. To avoid this behavior and this warning, we recommend you to overwrite the generation config model attribute before calling the model's `save_pretrained`, preferably also removing any generation kwargs from the model config. This warning will be raised to an exception in v4.41.\n",
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "Your generation config was originally created from the model config, but the model config has changed since then. Unless you pass the `generation_config` argument to this model's `generate` calls, they will revert to the legacy behavior where the base `generate` parameterization is loaded from the model config instead. To avoid this behavior and this warning, we recommend you to overwrite the generation config model attribute before calling the model's `save_pretrained`, preferably also removing any generation kwargs from the model config. This warning will be raised to an exception in v4.41.\n",
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "Your generation config was originally created from the model config, but the model config has changed since then. Unless you pass the `generation_config` argument to this model's `generate` calls, they will revert to the legacy behavior where the base `generate` parameterization is loaded from the model config instead. To avoid this behavior and this warning, we recommend you to overwrite the generation config model attribute before calling the model's `save_pretrained`, preferably also removing any generation kwargs from the model config. This warning will be raised to an exception in v4.41.\n",
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "Your generation config was originally created from the model config, but the model config has changed since then. Unless you pass the `generation_config` argument to this model's `generate` calls, they will revert to the legacy behavior where the base `generate` parameterization is loaded from the model config instead. To avoid this behavior and this warning, we recommend you to overwrite the generation config model attribute before calling the model's `save_pretrained`, preferably also removing any generation kwargs from the model config. This warning will be raised to an exception in v4.41.\n",
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "Your generation config was originally created from the model config, but the model config has changed since then. Unless you pass the `generation_config` argument to this model's `generate` calls, they will revert to the legacy behavior where the base `generate` parameterization is loaded from the model config instead. To avoid this behavior and this warning, we recommend you to overwrite the generation config model attribute before calling the model's `save_pretrained`, preferably also removing any generation kwargs from the model config. This warning will be raised to an exception in v4.41.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2500, training_loss=0.34207135314941406, metrics={'train_runtime': 933.7003, 'train_samples_per_second': 10.71, 'train_steps_per_second': 2.678, 'total_flos': 1190891520000000.0, 'train_loss': 0.34207135314941406, 'epoch': 5.0})"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df1['text_tagged'] = '<context>' + train_df1['context'] + '</context> ' + train_df1['text']\n",
    "core_ds, train_ds, val_ds = process_ds(train_df1,text_col='text_tagged')\n",
    "\n",
    "# To save memory and reset model to pre_trained version\n",
    "for obj in obj_del:\n",
    "    if obj in globals(): del globals()[obj]\n",
    "gc.collect()\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "model.config.decoder_start_token_id = model.config.bos_token_id\n",
    "model.config.pad_token_id = -100 # https://discuss.huggingface.co/t/expected-workflow-100-and-padding-in-labels-in-seq2seq/27692\n",
    "\n",
    "# Train\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    "    preprocess_logits_for_metrics=preprocess_logits_for_metrics\n",
    ")\n",
    "trainer.args.dataloader_num_workers = 0\n",
    "trainer.args.dataloader_prefetch_factor = 2\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ce198de5-1df1-444a-87ff-0eb6623fe448",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"{'bleu': 0.07231884241093674, 'precisions': [0.38619582664526486, 0.11972555746140652, 0.0427255985267035, 0.015109343936381709], 'brevity_penalty': 0.9784066913703916, 'length_ratio': 0.978636506440465, 'translation_length': 3115, 'reference_length': 3183}\" of type <class 'dict'> for key \"eval/bleu\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'rouge1': 0.3496136255443621, 'rouge2': 0.08951637098864798, 'rougeL': 0.32253719227843497, 'rougeLsum': 0.3216995145557498}\" of type <class 'dict'> for key \"eval/rouge\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.302137553691864,\n",
       " 'eval_bleu': {'bleu': 0.07231884241093674,\n",
       "  'precisions': [0.38619582664526486,\n",
       "   0.11972555746140652,\n",
       "   0.0427255985267035,\n",
       "   0.015109343936381709],\n",
       "  'brevity_penalty': 0.9784066913703916,\n",
       "  'length_ratio': 0.978636506440465,\n",
       "  'translation_length': 3115,\n",
       "  'reference_length': 3183},\n",
       " 'eval_rouge': {'rouge1': 0.3496136255443621,\n",
       "  'rouge2': 0.08951637098864798,\n",
       "  'rougeL': 0.32253719227843497,\n",
       "  'rougeLsum': 0.3216995145557498},\n",
       " 'eval_runtime': 7.0126,\n",
       " 'eval_samples_per_second': 28.52,\n",
       " 'eval_steps_per_second': 7.13,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.args.dataloader_num_workers = 0\n",
    "trainer.args.dataloader_prefetch_factor = 2 # To avoid HF error caused by setting prefetch_factor to None by default\n",
    "val_ds2 = core_ds['test'].select(range(200))\n",
    "trainer.evaluate(val_ds2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "41eade41-2418-42e1-88b5-d0ffbf4d1ac4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/generation/utils.py:1256: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/generation/utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: <context>disappointed</context> That must be incredibly annoying to have to deal with. To constantly have to battle for what you feel is justified support_comma_ and to always be worrying about it in the back of your head. You have to be really resilient to go through that. \n",
      "Generated response: Yeah_comma_ I have to deal with that every day. \n",
      "Actual response: Its been ten years and it doesnt even show up half of the time. I told the judge the last time I don't even want it anymore.\n"
     ]
    }
   ],
   "source": [
    "generate_sentences(model,0,train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "584ce695-a67f-40d5-88cd-52ab77a9d1b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: <context>angry</context> Not really. I hate confrontation. And nothing really of cost was lost. I'll just lose time in cleaning up after them. It definitely is a learning experience when it comes to trust... Doubt I'll be an AirBnB host any time soon! \n",
      "Generated response: Yeah_comma_ I know how you feel about that. \n",
      "Actual response: That's too bad. Lesson learned_comma_ I guess.\n"
     ]
    }
   ],
   "source": [
    "generate_sentences(model,12,train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640e1c9a-836e-4834-97f1-b139d7d8a9eb",
   "metadata": {},
   "source": [
    "**Discussion of Results:**\n",
    "\n",
    "*Without context labels:*\n",
    "\n",
    " {'eval_loss': 0.3026095926761627,\n",
    " 'eval_bleu': {'bleu': 0.07133228458970757,\n",
    "  'precisions': [0.38844042621892155,\n",
    "   0.11908871246116673,\n",
    "   0.04338153503893215,\n",
    "   0.014417300760913096],\n",
    "  'brevity_penalty': 0.9726132006914829,\n",
    "  'length_ratio': 0.9729814640276468,\n",
    "  'translation_length': 3097,\n",
    "  'reference_length': 3183},\n",
    " 'eval_rouge': {'rouge1': 0.35157515187169935,\n",
    "  'rouge2': 0.0897351333385348,\n",
    "  'rougeL': 0.32234644204153207,\n",
    "  'rougeLsum': 0.32162831666980013},\n",
    " 'eval_runtime': 6.7441,\n",
    " 'eval_samples_per_second': 29.656,\n",
    " 'eval_steps_per_second': 7.414,\n",
    " 'epoch': 5.0}\n",
    "\n",
    "*With context labels:*\n",
    "\n",
    "{'eval_loss': 0.302137553691864,\n",
    " 'eval_bleu': {'bleu': 0.07231884241093674,\n",
    "  'precisions': [0.38619582664526486,\n",
    "   0.11972555746140652,\n",
    "   0.0427255985267035,\n",
    "   0.015109343936381709],\n",
    "  'brevity_penalty': 0.9784066913703916,\n",
    "  'length_ratio': 0.978636506440465,\n",
    "  'translation_length': 3115,\n",
    "  'reference_length': 3183},\n",
    " 'eval_rouge': {'rouge1': 0.3496136255443621,\n",
    "  'rouge2': 0.08951637098864798,\n",
    "  'rougeL': 0.32253719227843497,\n",
    "  'rougeLsum': 0.3216995145557498},\n",
    " 'eval_runtime': 7.0126,\n",
    " 'eval_samples_per_second': 28.52,\n",
    " 'eval_steps_per_second': 7.13,\n",
    " 'epoch': 5.0}\n",
    " \n",
    "From the above, we observe that there are marginal increases and decreases in the bleu and rouge scores respectively, though the differences are too small for a conclusive comparison to be made. This is interesting as it suggests that this context information contributes a slightly noisy signal. \n",
    "\n",
    "Qualitatively, the responses are very close to each other, e.g. \"Yeah_comma_ I have to deal with that every day.\" vs \"I agree_comma_ I have to deal with that every day.\". This might be due to the fact that this is a very new token to the model that is being trained with very short-form text, thus the contextual implications of this tag could not be very accurately captured with a small dataset size (I am limited, in this case, by the 2x16GB GPUs available to me on Vertex as well as time). In addition, the contextual relevance of the word 'anger', or any emotional context for that matter, may be quite nuanced and thus not be easily inferrable. While it refers to the emotional state of the affected person, the appropriate/empathetic response would not necessarily be to reciprocate with the same emotion (e.g. anger on anger certainly does not help).\n",
    "\n",
    "Possible adjustments to try include using other 'token types' to replace <context>, as well as the inclusion of more layers or attention heads to extract the nuanced relationship between the affected person's emotional state and the appropriate emotions to respond with. Another possible vector of analysis for this variable is a deep-dive into the attention weights, although they are difficult to interpret due to the use of multiple attention heads and the intervening feed-forward layers.|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aaa6a27-1ce4-4bbc-a130-86e7126286ee",
   "metadata": {},
   "source": [
    "### 5.2 Hyperparameter: Increasing Attention Head Count\n",
    "**Hypothesis:**\n",
    "\n",
    "The intuition here is quite similar to what was discussed above - the relationship between the affected person's emotional state and the appropriate sentiment/attitude to respond with is quite nuanced. For example, it might be best to mirror the other person in some cases, but portray a different front in others. As such, I have increased the attention head count from 12 to 15 for the decoder layers, to see if more diverse relationships can be captured between the input and output tokens. Another possible intervention could be to increase the layer count on the decoder end, which is omitted from this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fea70ff5-2661-4642-9ba9-6456355299f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2500' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2500/2500 15:35, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.143000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.104600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.083200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.073500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.075800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2500, training_loss=0.09601026458740235, metrics={'train_runtime': 935.4861, 'train_samples_per_second': 10.69, 'train_steps_per_second': 2.672, 'total_flos': 1190891520000000.0, 'train_loss': 0.09601026458740235, 'epoch': 5.0})"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "core_ds, train_ds, val_ds = process_ds(train_df1)\n",
    "\n",
    "# To save memory and reset model to pre_trained version\n",
    "for obj in obj_del:\n",
    "    if obj in globals(): del globals()[obj]\n",
    "gc.collect()\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "model.config.decoder_start_token_id = model.config.bos_token_id\n",
    "model.config.pad_token_id = -100 # https://discuss.huggingface.co/t/expected-workflow-100-and-padding-in-labels-in-seq2seq/27692\n",
    "model.config.decoder_attention_heads = 15\n",
    "\n",
    "trainer2 = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    "    preprocess_logits_for_metrics=preprocess_logits_for_metrics\n",
    ")\n",
    "trainer2.args.dataloader_num_workers = 0\n",
    "trainer2.args.dataloader_prefetch_factor = 2\n",
    "trainer2.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6daa9829-9d71-4dab-b78d-1a66a44f27cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"{'bleu': 0.06544166604677476, 'precisions': [0.35840565734490515, 0.09859154929577464, 0.03762449280708226, 0.015133412982875348], 'brevity_penalty': 0.977122077168128, 'length_ratio': 0.9773798303487277, 'translation_length': 3111, 'reference_length': 3183}\" of type <class 'dict'> for key \"eval/bleu\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'rouge1': 0.3214492176332781, 'rouge2': 0.06996314708058868, 'rougeL': 0.2909687335888218, 'rougeLsum': 0.290597067604345}\" of type <class 'dict'> for key \"eval/rouge\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.3747697174549103,\n",
       " 'eval_bleu': {'bleu': 0.06544166604677476,\n",
       "  'precisions': [0.35840565734490515,\n",
       "   0.09859154929577464,\n",
       "   0.03762449280708226,\n",
       "   0.015133412982875348],\n",
       "  'brevity_penalty': 0.977122077168128,\n",
       "  'length_ratio': 0.9773798303487277,\n",
       "  'translation_length': 3111,\n",
       "  'reference_length': 3183},\n",
       " 'eval_rouge': {'rouge1': 0.3214492176332781,\n",
       "  'rouge2': 0.06996314708058868,\n",
       "  'rougeL': 0.2909687335888218,\n",
       "  'rougeLsum': 0.290597067604345},\n",
       " 'eval_runtime': 6.7513,\n",
       " 'eval_samples_per_second': 29.624,\n",
       " 'eval_steps_per_second': 7.406,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer2.args.dataloader_num_workers = 0\n",
    "trainer2.args.dataloader_prefetch_factor = 2 # To avoid HF error caused by setting prefetch_factor to None by default\n",
    "val_ds2 = core_ds['test'].select(range(200))\n",
    "trainer2.evaluate(val_ds2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "343a9107-3f24-46d2-95b5-dd2e92fbc2e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/generation/utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: That must be incredibly annoying to have to deal with. To constantly have to battle for what you feel is justified support_comma_ and to always be worrying about it in the back of your head. You have to be really resilient to go through that. \n",
      "Generated response: I feel like I have to battle for what I feel is right. \n",
      "Actual response: Its been ten years and it doesnt even show up half of the time. I told the judge the last time I don't even want it anymore.\n"
     ]
    }
   ],
   "source": [
    "generate_sentences(model,0,train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f0e4d161-6e9d-4e9c-8af3-bb8f9c8c2939",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Those easy semesters help when you know you've got some harder ones on the horizon.  Good Luck! \n",
      "Generated response: Yeah hopefully they can make it out alive. \n",
      "Actual response: Yep. I hope so\n"
     ]
    }
   ],
   "source": [
    "generate_sentences(model,1,val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7e7a98-d1e5-4d15-a24a-1d3dea5c2e73",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Discussion of Results**\n",
    "\n",
    "*12 attention heads:*\n",
    "\n",
    "{'eval_loss': 0.3026095926761627,\n",
    " 'eval_bleu': {'bleu': 0.07133228458970757,\n",
    "  'precisions': [0.38844042621892155,\n",
    "   0.11908871246116673,\n",
    "   0.04338153503893215,\n",
    "   0.014417300760913096],\n",
    "  'brevity_penalty': 0.9726132006914829,\n",
    "  'length_ratio': 0.9729814640276468,\n",
    "  'translation_length': 3097,\n",
    "  'reference_length': 3183},\n",
    " 'eval_rouge': {'rouge1': 0.35157515187169935,\n",
    "  'rouge2': 0.0897351333385348,\n",
    "  'rougeL': 0.32234644204153207,\n",
    "  'rougeLsum': 0.32162831666980013},\n",
    " 'eval_runtime': 6.7441,\n",
    " 'eval_samples_per_second': 29.656,\n",
    " 'eval_steps_per_second': 7.414,\n",
    " 'epoch': 5.0}\n",
    "\n",
    " *15 attention heads:*\n",
    " \n",
    "{'eval_loss': 0.3747697174549103,\n",
    " 'eval_bleu': {'bleu': 0.06544166604677476,\n",
    "  'precisions': [0.35840565734490515,\n",
    "   0.09859154929577464,\n",
    "   0.03762449280708226,\n",
    "   0.015133412982875348],\n",
    "  'brevity_penalty': 0.977122077168128,\n",
    "  'length_ratio': 0.9773798303487277,\n",
    "  'translation_length': 3111,\n",
    "  'reference_length': 3183},\n",
    " 'eval_rouge': {'rouge1': 0.3214492176332781,\n",
    "  'rouge2': 0.06996314708058868,\n",
    "  'rougeL': 0.2909687335888218,\n",
    "  'rougeLsum': 0.290597067604345},\n",
    " 'eval_runtime': 6.7513,\n",
    " 'eval_samples_per_second': 29.624,\n",
    " 'eval_steps_per_second': 7.406,\n",
    " 'epoch': 5.0}\n",
    " \n",
    "From the above results, we observe that the model performs significantly worse across all metrics when more attention heads are used, which might suggest some degree of overfitting. This is not surprising as the preset hyperparameters had already been fine-tuned during pretraining. Another possible cause of overfitting could be that the input and output texts for this task are generally quite short in length, thus having more attention heads fails to capture any kinds of generalizable inter-word/phrase relationships. As such, other interventions are needed to capture unrepresented nuances, particularly interventions on the data- and feature-generation level."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cc145d-410b-4695-bf71-a3605e2484a0",
   "metadata": {},
   "source": [
    "### 5.3 Visual Comparison of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d03a8f0e-18c5-4a49-96f0-38955986e557",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig,(ax1,ax2) = plt.subplots(1,2,figsize=(15,5))\n",
    "bleu_results = [0.0026524365368443847,0.07133228458970757,0.07231884241093674, 0.06544166604677476]\n",
    "rouge_results = [0.1318835069428388,0.35157515187169935,0.3496136255443621,0.3214492176332781]\n",
    "labels = ['pretrained','control','context labels','15 attn heads']\n",
    "ax1.bar(['pretrained','control','context labels','15 attn heads'],bleu_results,label=labels)\n",
    "ax1.set_title('Bleu')\n",
    "ax2.bar(['pretrained','control','context labels','15 attn heads'],rouge_results,label=labels)\n",
    "ax2.set_title('Rouge')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4944cf40-7a53-4949-bdf9-a82d8c17bc92",
   "metadata": {},
   "source": [
    "## 6. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccb7b83-a87e-4068-8038-e9bf00a81995",
   "metadata": {},
   "source": [
    "Overall, the pretrained BART model fares poorly on the task of generating empathetic responses, but the finetuned BART model performs significantly better. There is, however, significant room for improvement, which is non-trivial to realize due to the complex nature of empathy and what an empathetic response might entail in the context of a multi-turn dialogue. On this note, the following are possible directions for future work/research:\n",
    "- Using alternative metrics that capture more nuances than Bleu and Rouge, e.g. MoverScore and BERTScore.\n",
    "- Using multiple preceding turns, i.e. not just y(t-1), as input, to capture more nuances in how the each turn of dialogue relates semantically to the overall context of the dialogue as well as its preceding turns.\n",
    "- Fine-tuning a decoder-only architecture, with the entire preceding part of the dialogue as input.\n",
    "- Experimenting with a different S2S model, e.g. T5, or a bert-based S2S architecture.\n",
    "- Tuning other hyperparameters, e.g. layer count and hidden node count. \n",
    "- Secure resources to train model on larger sample size\n",
    "- Leveraging other existing literature for ideas, e.g. applying an additional annotation/tag with a cognitively reasoned inference about the affected person's emotional state, which is an intuition used in Chain-of-Empathy prompting (Lee et al., 2023)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d690193",
   "metadata": {},
   "source": [
    "## 7. References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c0e4d5",
   "metadata": {},
   "source": [
    "- Lee, Y. K., Lee, I., Shin, M., Bae, S., & Hahn, S. (2023). Chain of Empathy: Enhancing Empathetic Response of Large Language Models Based on Psychotherapy Models. arXiv. \n",
    "- Lewis, M., Liu, Y., Goyal, N.,  Ghazvininejad, M., Mohamed, A., Levy, O., Stoyanov, V., & Zettlemoyer, L. (2019). BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, \n",
    "- Rashkin, H., Smith, E. M., Li, M., & Boureau, Y.-L. (2018, November 1). Towards Empathetic Open-domain Conversation Models: a New Benchmark and Dataset. Translation, and Comprehension. arXiv. "
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-pytorch-pytorch",
   "name": "workbench-notebooks.m119",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m119"
  },
  "kernelspec": {
   "display_name": "PyTorch 1-13 (Local)",
   "language": "python",
   "name": "conda-env-pytorch-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
